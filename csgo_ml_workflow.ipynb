{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CST3133 - Traditional Machine Learning Workflow and Ethics\n",
    "## CSGO Match Prediction Dataset\n",
    "\n",
    "This notebook implements the complete machine learning workflow for predicting CSGO match outcomes using structured data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Dataset Selection and Problem Definition (5%)\n",
    "\n",
    "### Dataset Overview\n",
    "We are using a **CSGO (Counter-Strike: Global Offensive) match dataset** that contains historical match data with team statistics and player performance metrics.\n",
    "\n",
    "**Dataset Source:** Publicly available CSGO match dataset\n",
    "**Dataset Link:** [To be provided in report]\n",
    "\n",
    "### Problem Definition\n",
    "**Problem Type:** **Classification Problem**\n",
    "\n",
    "We aim to predict the **match winner** (Team 1 or Team 2) based on:\n",
    "- Team rankings and historical performance\n",
    "- Individual player statistics (ratings, KDR, impact, etc.)\n",
    "- Head-to-head win percentages\n",
    "- Match points\n",
    "\n",
    "**Target Variable:** `winner` (categorical: 't1' or 't2')\n",
    "\n",
    "**Features:** \n",
    "- Team-level features: rankings, points, head-to-head statistics\n",
    "- Player-level features: ratings, KDR (Kill-Death Ratio), impact, damage, assists, etc. for all 10 players (5 per team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "**For Google Colab (Recommended):**\n",
    "- No virtual environment needed! Google Colab comes with all required packages pre-installed.\n",
    "- Simply upload this notebook and the `csgo_games.csv` file to Google Colab.\n",
    "- If any package is missing, uncomment and run the cell below.\n",
    "\n",
    "**For Local Development (Optional):**\n",
    "- If running locally, you can create a virtual environment:\n",
    "  ```bash\n",
    "  python -m venv venv\n",
    "  source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "  pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed (uncomment if running in Google Colab and packages are missing)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "# !pip install xgboost  # Optional: For advanced boosting models (will be checked automatically)\n",
    "# !pip install lightgbm  # Optional: Alternative gradient boosting (will be checked automatically)\n",
    "\n",
    "# Note: This cell should be run AFTER the imports cell below\n",
    "# Verification will happen after imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Verify installations\n",
    "import sys\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "except:\n",
    "    print(\"Scikit-learn: Installed\")\n",
    "print(\"\\nâœ“ All required packages are available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('csgo_games.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(df['winner'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Basic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Data Preprocessing (10%)\n",
    "\n",
    "### Step 1: Handle Missing Values\n",
    "We will identify and handle missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Percentage': missing_percentage.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing values\n",
    "if len(missing_df) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(missing_df['Column'], missing_df['Percentage'])\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce noise to the dataset (as per requirements)\n",
    "# We'll introduce random NaN values to some columns to demonstrate noise handling\n",
    "np.random.seed(42)\n",
    "df_noisy = df.copy()\n",
    "\n",
    "# Select some numerical columns to introduce noise\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Exclude target variable and key identifiers\n",
    "cols_to_noise = [col for col in numerical_cols if col not in ['t1_points', 't2_points']]\n",
    "\n",
    "# Introduce 5% random missing values to selected columns\n",
    "for col in cols_to_noise[:10]:  # Add noise to first 10 numerical columns\n",
    "    n_missing = int(len(df_noisy) * 0.05)\n",
    "    missing_indices = np.random.choice(df_noisy.index, n_missing, replace=False)\n",
    "    df_noisy.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"Noise introduced. New missing values:\")\n",
    "print(df_noisy.isnull().sum().sum(), \"total missing values\")\n",
    "print(\"\\nColumns with introduced noise:\")\n",
    "noisy_cols = df_noisy.isnull().sum()[df_noisy.isnull().sum() > 0].index.tolist()\n",
    "print(noisy_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical columns: use median imputation\n",
    "# For categorical columns: use mode imputation\n",
    "\n",
    "df_processed = df_noisy.copy()\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Impute numerical columns with median\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "df_processed[numerical_cols] = numerical_imputer.fit_transform(df_processed[numerical_cols])\n",
    "\n",
    "# Impute categorical columns with mode\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        mode_value = df_processed[col].mode()[0] if len(df_processed[col].mode()) > 0 else 'Unknown'\n",
    "        df_processed[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "print(\"Missing values after imputation:\", df_processed.isnull().sum().sum())\n",
    "print(\"\\nImputation Strategy:\")\n",
    "print(\"- Numerical columns: Median imputation\")\n",
    "print(\"- Categorical columns: Mode imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering and Encoding\n",
    "We will create logical features and encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create aggregated team statistics\n",
    "df_features = df_processed.copy()\n",
    "\n",
    "# Calculate average team ratings\n",
    "t1_rating_cols = [col for col in df_features.columns if 't1_player' in col and 'rating' in col]\n",
    "t2_rating_cols = [col for col in df_features.columns if 't2_player' in col and 'rating' in col]\n",
    "\n",
    "df_features['t1_avg_rating'] = df_features[t1_rating_cols].mean(axis=1)\n",
    "df_features['t2_avg_rating'] = df_features[t2_rating_cols].mean(axis=1)\n",
    "\n",
    "# Calculate average team impact\n",
    "t1_impact_cols = [col for col in df_features.columns if 't1_player' in col and 'impact' in col]\n",
    "t2_impact_cols = [col for col in df_features.columns if 't2_player' in col and 'impact' in col]\n",
    "\n",
    "df_features['t1_avg_impact'] = df_features[t1_impact_cols].mean(axis=1)\n",
    "df_features['t2_avg_impact'] = df_features[t2_impact_cols].mean(axis=1)\n",
    "\n",
    "# Calculate average team KDR\n",
    "t1_kdr_cols = [col for col in df_features.columns if 't1_player' in col and 'kdr' in col]\n",
    "t2_kdr_cols = [col for col in df_features.columns if 't2_player' in col and 'kdr' in col]\n",
    "\n",
    "df_features['t1_avg_kdr'] = df_features[t1_kdr_cols].mean(axis=1)\n",
    "df_features['t2_avg_kdr'] = df_features[t2_kdr_cols].mean(axis=1)\n",
    "\n",
    "# Rank difference (lower rank number = better team)\n",
    "df_features['rank_diff'] = df_features['t1_world_rank'] - df_features['t2_world_rank']\n",
    "\n",
    "# Points difference - REMOVED (data leakage: points are known after match)\n",
    "# df_features['points_diff'] = df_features['t1_points'] - df_features['t2_points']\n",
    "\n",
    "# H2H advantage\n",
    "df_features['h2h_advantage'] = df_features['t1_h2h_win_perc'] - df_features['t2_h2h_win_perc']\n",
    "\n",
    "print(\"New engineered features created:\")\n",
    "new_features = ['t1_avg_rating', 't2_avg_rating', 't1_avg_impact', 't2_avg_impact', \n",
    "                't1_avg_kdr', 't2_avg_kdr', 'rank_diff', 'h2h_advantage']\n",
    "print(new_features)\n",
    "print(\"\\nSample of engineered features:\")\n",
    "df_features[new_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "# First, check and filter the winner column to ensure binary classification\n",
    "print(\"Original winner column unique values:\", df_features['winner'].unique())\n",
    "print(\"Original winner column value counts:\")\n",
    "print(df_features['winner'].value_counts())\n",
    "\n",
    "# Filter to only keep 't1' and 't2' if there are other values\n",
    "df_features = df_features[df_features['winner'].isin(['t1', 't2'])].copy()\n",
    "print(f\"\\nAfter filtering: {len(df_features)} rows remaining\")\n",
    "\n",
    "# Select relevant features (exclude identifiers and target)\n",
    "exclude_cols = ['match_date', 'team_1', 'team_2', 'winner']\n",
    "feature_cols = [col for col in df_features.columns if col not in exclude_cols]\n",
    "\n",
    "# Separate features and target\n",
    "X = df_features[feature_cols]\n",
    "y = df_features['winner']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Handle boolean columns (convert to int)\n",
    "bool_cols = X.select_dtypes(include=['bool']).columns\n",
    "X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "# Ensure all features are numerical\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Remove data leakage features (these contain information only known after the match)\n",
    "leakage_features = ['t1_points', 't2_points', 'points_diff']\n",
    "X = X.drop(columns=[col for col in leakage_features if col in X.columns], errors='ignore')\n",
    "print(f\"Removed data leakage features. Remaining features: {X.shape[1]}\")\n",
    "print(f\"Removed features: {[col for col in leakage_features if col in df_features.columns]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y_encoded.shape}\")\n",
    "print(f\"\\nTarget encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale/Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"Scaled features shape: {X_scaled.shape}\")\n",
    "print(\"\\nSample of scaled features:\")\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Exploratory Data Analysis (10%)\n",
    "\n",
    "### Visualizations and Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "y.value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
    "plt.title('Distribution of Match Winners')\n",
    "plt.xlabel('Winner')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Winner Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass Balance: {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of key features\n",
    "key_features = ['t1_world_rank', 't2_world_rank', 't1_avg_rating', 't2_avg_rating', \n",
    "                't1_avg_kdr', 't2_avg_kdr', 'rank_diff', 'h2h_advantage']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    if feature in df_features.columns:\n",
    "        axes[i].hist(df_features[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "# Select a subset of features for correlation analysis\n",
    "# Note: Removed t1_points, t2_points, points_diff (data leakage)\n",
    "correlation_features = ['t1_world_rank', 't2_world_rank',\n",
    "                        't1_h2h_win_perc', 't2_h2h_win_perc', 't1_avg_rating', 't2_avg_rating',\n",
    "                        't1_avg_kdr', 't2_avg_kdr', 'rank_diff', 'h2h_advantage']\n",
    "\n",
    "correlation_features = [f for f in correlation_features if f in df_features.columns]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_features[correlation_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Key Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Correlations:\")\n",
    "print(\"Strong positive correlations indicate features that move together.\")\n",
    "print(\"Strong negative correlations indicate inverse relationships.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between rank difference and winner\n",
    "if 'rank_diff' in df_features.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df_features.boxplot(column='rank_diff', by='winner', ax=plt.gca())\n",
    "    plt.title('Rank Difference by Winner')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    plt.xlabel('Winner')\n",
    "    plt.ylabel('Rank Difference (t1_rank - t2_rank)')\n",
    "    \n",
    "    # Violin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.violinplot(data=df_features, x='winner', y='rank_diff')\n",
    "    plt.title('Rank Difference Distribution by Winner')\n",
    "    plt.xlabel('Winner')\n",
    "    plt.ylabel('Rank Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInsight: Negative rank_diff means Team 1 has better rank (lower number).\")\n",
    "    print(\"Positive rank_diff means Team 2 has better rank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze average ratings by winner\n",
    "if 't1_avg_rating' in df_features.columns and 't2_avg_rating' in df_features.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Team 1 average rating by winner\n",
    "    axes[0].scatter(df_features['t1_avg_rating'], \n",
    "                   [1 if w == 't1' else 0 for w in df_features['winner']], \n",
    "                   alpha=0.5, label='Team 1 Wins')\n",
    "    axes[0].scatter(df_features['t1_avg_rating'], \n",
    "                   [1 if w == 't2' else 0 for w in df_features['winner']], \n",
    "                   alpha=0.5, label='Team 2 Wins')\n",
    "    axes[0].set_xlabel('Team 1 Average Rating')\n",
    "    axes[0].set_ylabel('Winner (1=t1, 0=t2)')\n",
    "    axes[0].set_title('Team 1 Rating vs Winner')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Team 2 average rating by winner\n",
    "    axes[1].scatter(df_features['t2_avg_rating'], \n",
    "                   [1 if w == 't2' else 0 for w in df_features['winner']], \n",
    "                   alpha=0.5, label='Team 2 Wins')\n",
    "    axes[1].scatter(df_features['t2_avg_rating'], \n",
    "                   [1 if w == 't1' else 0 for w in df_features['winner']], \n",
    "                   alpha=0.5, label='Team 1 Wins')\n",
    "    axes[1].set_xlabel('Team 2 Average Rating')\n",
    "    axes[1].set_ylabel('Winner (1=t2, 0=t1)')\n",
    "    axes[1].set_title('Team 2 Rating vs Winner')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Trends and Patterns\n",
    "\n",
    "1. **Class Distribution**: The dataset shows [describe balance/imbalance]\n",
    "2. **Rank Impact**: Teams with better rankings (lower numbers) tend to win more often\n",
    "3. **Rating Correlation**: Higher average player ratings correlate with match wins\n",
    "4. **Feature Relationships**: [Describe key correlations found]\n",
    "\n",
    "### Potential Biases\n",
    "\n",
    "1. **Temporal Bias**: Matches from different time periods may have different meta-games\n",
    "2. **Team Representation**: Some teams may be over/under-represented\n",
    "3. **Rank Bias**: Rankings may not reflect current team strength accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Model Development and Evaluation (15%)\n",
    "\n",
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Check unique values to ensure binary classification\n",
    "print(f\"Unique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred_lr: {np.unique(y_pred_lr)}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_test))}\")\n",
    "\n",
    "# Evaluation - use appropriate average based on number of classes\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    # Multiclass: use 'macro' average\n",
    "    lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "    lr_precision = precision_score(y_test, y_pred_lr, average='macro', zero_division=0)\n",
    "    lr_recall = recall_score(y_test, y_pred_lr, average='macro', zero_division=0)\n",
    "    lr_f1 = f1_score(y_test, y_pred_lr, average='macro', zero_division=0)\n",
    "    print(\"\\nâš ï¸  Note: Using multiclass metrics (macro average) because target has more than 2 classes\")\n",
    "else:\n",
    "    # Binary classification\n",
    "    lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "    lr_precision = precision_score(y_test, y_pred_lr, average='binary', zero_division=0)\n",
    "    lr_recall = recall_score(y_test, y_pred_lr, average='binary', zero_division=0)\n",
    "    lr_f1 = f1_score(y_test, y_pred_lr, average='binary', zero_division=0)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\")\n",
    "print(f\"F1-Score: {lr_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Team 2', 'Team 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Team 2', 'Team 1'], \n",
    "            yticklabels=['Team 2', 'Team 1'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Check unique values\n",
    "print(f\"Unique values in y_test: {np.unique(y_test)}\")\n",
    "print(f\"Unique values in y_pred_dt: {np.unique(y_pred_dt)}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_test))}\")\n",
    "\n",
    "# Evaluation - use appropriate average based on number of classes\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    # Multiclass: use 'macro' average\n",
    "    dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "    dt_precision = precision_score(y_test, y_pred_dt, average='macro', zero_division=0)\n",
    "    dt_recall = recall_score(y_test, y_pred_dt, average='macro', zero_division=0)\n",
    "    dt_f1 = f1_score(y_test, y_pred_dt, average='macro', zero_division=0)\n",
    "    print(\"\\nâš ï¸  Note: Using multiclass metrics (macro average) because target has more than 2 classes\")\n",
    "else:\n",
    "    # Binary classification\n",
    "    dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "    dt_precision = precision_score(y_test, y_pred_dt, average='binary', zero_division=0)\n",
    "    dt_recall = recall_score(y_test, y_pred_dt, average='binary', zero_division=0)\n",
    "    dt_f1 = f1_score(y_test, y_pred_dt, average='binary', zero_division=0)\n",
    "\n",
    "print(\"\\nDecision Tree Results:\")\n",
    "print(f\"Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Precision: {dt_precision:.4f}\")\n",
    "print(f\"Recall: {dt_recall:.4f}\")\n",
    "print(f\"F1-Score: {dt_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=['Team 2', 'Team 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Decision Tree\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances - Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Decision Tree\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Team 2', 'Team 1'], \n",
    "            yticklabels=['Team 2', 'Team 1'])\n",
    "plt.title('Confusion Matrix - Decision Tree')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: K-Means Clustering (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering (using 2 clusters to match binary classification)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate clustering by comparing with actual labels\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "ari_score = adjusted_rand_score(y_encoded, clusters)\n",
    "silhouette = silhouette_score(X_scaled, clusters)\n",
    "\n",
    "print(\"K-Means Clustering Results:\")\n",
    "print(f\"Adjusted Rand Index: {ari_score:.4f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"\\nCluster distribution: {pd.Series(clusters).value_counts().sort_index()}\")\n",
    "print(f\"Actual class distribution: {pd.Series(y_encoded).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Actual labels\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter1 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter1, label='Actual Class')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Actual Classes (PCA Visualization)')\n",
    "\n",
    "# Plot 2: Predicted clusters\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter2 = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='plasma', alpha=0.6)\n",
    "plt.colorbar(scatter2, label='Cluster')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('K-Means Clusters (PCA Visualization)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total Explained Variance: {sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4.1 Model Improvements (Optional Enhancement)\n",
    "\n",
    "This section explores advanced models and techniques to improve prediction accuracy beyond the baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model 1: Random Forest\n",
    "\n",
    "Random Forest combines multiple decision trees to reduce overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    rf_precision = precision_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
    "    rf_recall = recall_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
    "    rf_f1 = f1_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
    "else:\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    rf_precision = precision_score(y_test, y_pred_rf, average='binary', zero_division=0)\n",
    "    rf_recall = recall_score(y_test, y_pred_rf, average='binary', zero_division=0)\n",
    "    rf_f1 = f1_score(y_test, y_pred_rf, average='binary', zero_division=0)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall: {rf_recall:.4f}\")\n",
    "print(f\"F1-Score: {rf_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Team 2', 'Team 1', 'Other'] if len(np.unique(y_test)) > 2 else ['Team 2', 'Team 1']))\n",
    "\n",
    "# Feature importance\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(rf_feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model 2: Feature Selection\n",
    "\n",
    "Selecting the most important features can improve model performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using Random Forest importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Select top features based on Random Forest importance\n",
    "selector = SelectFromModel(rf_model, prefit=True, max_features=50)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(f\"Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n",
    "print(f\"\\nTop 20 Selected Features:\")\n",
    "for i, feat in enumerate(selected_features[:20], 1):\n",
    "    print(f\"{i}. {feat}\")\n",
    "\n",
    "# Retrain Logistic Regression with selected features\n",
    "lr_selected = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_selected.fit(X_train_selected, y_train)\n",
    "y_pred_lr_selected = lr_selected.predict(X_test_selected)\n",
    "\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    lr_selected_accuracy = accuracy_score(y_test, y_pred_lr_selected)\n",
    "    lr_selected_precision = precision_score(y_test, y_pred_lr_selected, average='macro', zero_division=0)\n",
    "    lr_selected_recall = recall_score(y_test, y_pred_lr_selected, average='macro', zero_division=0)\n",
    "    lr_selected_f1 = f1_score(y_test, y_pred_lr_selected, average='macro', zero_division=0)\n",
    "else:\n",
    "    lr_selected_accuracy = accuracy_score(y_test, y_pred_lr_selected)\n",
    "    lr_selected_precision = precision_score(y_test, y_pred_lr_selected, average='binary', zero_division=0)\n",
    "    lr_selected_recall = recall_score(y_test, y_pred_lr_selected, average='binary', zero_division=0)\n",
    "    lr_selected_f1 = f1_score(y_test, y_pred_lr_selected, average='binary', zero_division=0)\n",
    "\n",
    "print(f\"\\nLogistic Regression with Feature Selection:\")\n",
    "print(f\"Accuracy: {lr_selected_accuracy:.4f} ({lr_selected_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {lr_selected_precision:.4f}\")\n",
    "print(f\"Recall: {lr_selected_recall:.4f}\")\n",
    "print(f\"F1-Score: {lr_selected_f1:.4f}\")\n",
    "print(f\"\\nImprovement: {((lr_selected_accuracy - lr_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Model 3: Hyperparameter Tuning\n",
    "\n",
    "Using Grid Search to find optimal hyperparameters for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid (using a smaller grid for faster execution)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [10, 20]\n",
    "}\n",
    "\n",
    "print(\"Performing Grid Search (this may take a few minutes)...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_rf_tuned = rf_grid.predict(X_test)\n",
    "\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    rf_tuned_accuracy = accuracy_score(y_test, y_pred_rf_tuned)\n",
    "    rf_tuned_precision = precision_score(y_test, y_pred_rf_tuned, average='macro', zero_division=0)\n",
    "    rf_tuned_recall = recall_score(y_test, y_pred_rf_tuned, average='macro', zero_division=0)\n",
    "    rf_tuned_f1 = f1_score(y_test, y_pred_rf_tuned, average='macro', zero_division=0)\n",
    "else:\n",
    "    rf_tuned_accuracy = accuracy_score(y_test, y_pred_rf_tuned)\n",
    "    rf_tuned_precision = precision_score(y_test, y_pred_rf_tuned, average='binary', zero_division=0)\n",
    "    rf_tuned_recall = recall_score(y_test, y_pred_rf_tuned, average='binary', zero_division=0)\n",
    "    rf_tuned_f1 = f1_score(y_test, y_pred_rf_tuned, average='binary', zero_division=0)\n",
    "\n",
    "print(f\"\\nTuned Random Forest Results:\")\n",
    "print(f\"Test Accuracy: {rf_tuned_accuracy:.4f} ({rf_tuned_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {rf_tuned_precision:.4f}\")\n",
    "print(f\"Recall: {rf_tuned_recall:.4f}\")\n",
    "print(f\"F1-Score: {rf_tuned_f1:.4f}\")\n",
    "print(f\"\\nImprovement over baseline RF: {((rf_tuned_accuracy - rf_accuracy) * 100):.2f}%\")\n",
    "print(f\"Improvement over Logistic Regression: {((rf_tuned_accuracy - lr_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models including improvements\n",
    "improved_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (Baseline)', 'Decision Tree (Baseline)', \n",
    "              'Random Forest', 'LR with Feature Selection', 'Tuned Random Forest'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy, rf_accuracy, \n",
    "                 lr_selected_accuracy if 'lr_selected_accuracy' in globals() else np.nan,\n",
    "                 rf_tuned_accuracy if 'rf_tuned_accuracy' in globals() else np.nan],\n",
    "    'Precision': [lr_precision, dt_precision, rf_precision,\n",
    "                  lr_selected_precision if 'lr_selected_precision' in globals() else np.nan,\n",
    "                  rf_tuned_precision if 'rf_tuned_precision' in globals() else np.nan],\n",
    "    'Recall': [lr_recall, dt_recall, rf_recall,\n",
    "               lr_selected_recall if 'lr_selected_recall' in globals() else np.nan,\n",
    "               rf_tuned_recall if 'rf_tuned_recall' in globals() else np.nan],\n",
    "    'F1-Score': [lr_f1, dt_f1, rf_f1,\n",
    "                 lr_selected_f1 if 'lr_selected_f1' in globals() else np.nan,\n",
    "                 rf_tuned_f1 if 'rf_tuned_f1' in globals() else np.nan]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPROVED MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(improved_comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "models_list = improved_comparison['Model'].tolist()\n",
    "accuracies = improved_comparison['Accuracy'].tolist()\n",
    "colors = ['skyblue', 'lightgreen', 'orange', 'purple', 'red']\n",
    "\n",
    "axes[0].barh(models_list, accuracies, color=colors[:len(models_list)])\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison (All Models)')\n",
    "axes[0].set_xlim([0.45, max([a for a in accuracies if not np.isnan(a)]) * 1.1])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, acc in enumerate(accuracies):\n",
    "    if not np.isnan(acc):\n",
    "        axes[0].text(acc + 0.005, i, f'{acc:.3f}', va='center')\n",
    "\n",
    "# Metrics comparison for top 3 models\n",
    "top_models = improved_comparison.nlargest(3, 'Accuracy')\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, (idx, row) in enumerate(top_models.iterrows()):\n",
    "    model_metrics = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]\n",
    "    axes[1].bar(x + i*width, model_metrics, width, label=row['Model'][:20], alpha=0.8)\n",
    "\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Top 3 Models - Detailed Metrics')\n",
    "axes[1].set_xticks(x + width)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = improved_comparison['Accuracy'].idxmax()\n",
    "best_model_name = improved_comparison.loc[best_model_idx, 'Model']\n",
    "best_model_acc = improved_comparison.loc[best_model_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   Improvement over baseline LR: {((best_model_acc - lr_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Feature Engineering Ideas\n",
    "\n",
    "Here are some additional features that could potentially improve model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Feature Engineering (Optional - for experimentation)\n",
    "# These features could be added to potentially improve performance\n",
    "\n",
    "df_enhanced = df_features.copy()\n",
    "\n",
    "# 1. Team strength difference (using average ratings)\n",
    "if 't1_avg_rating' in df_enhanced.columns and 't2_avg_rating' in df_enhanced.columns:\n",
    "    df_enhanced['rating_diff'] = df_enhanced['t1_avg_rating'] - df_enhanced['t2_avg_rating']\n",
    "\n",
    "# 2. Impact difference\n",
    "if 't1_avg_impact' in df_enhanced.columns and 't2_avg_impact' in df_enhanced.columns:\n",
    "    df_enhanced['impact_diff'] = df_enhanced['t1_avg_impact'] - df_enhanced['t2_avg_impact']\n",
    "\n",
    "# 3. KDR difference\n",
    "if 't1_avg_kdr' in df_enhanced.columns and 't2_avg_kdr' in df_enhanced.columns:\n",
    "    df_enhanced['kdr_diff'] = df_enhanced['t1_avg_kdr'] - df_enhanced['t2_avg_kdr']\n",
    "\n",
    "# 4. Team consistency (standard deviation of player ratings)\n",
    "t1_rating_cols = [col for col in df_enhanced.columns if 't1_player' in col and 'rating' in col]\n",
    "t2_rating_cols = [col for col in df_enhanced.columns if 't2_player' in col and 'rating' in col]\n",
    "\n",
    "if len(t1_rating_cols) > 0 and len(t2_rating_cols) > 0:\n",
    "    df_enhanced['t1_rating_std'] = df_enhanced[t1_rating_cols].std(axis=1)\n",
    "    df_enhanced['t2_rating_std'] = df_enhanced[t2_rating_cols].std(axis=1)\n",
    "    df_enhanced['rating_consistency_diff'] = df_enhanced['t1_rating_std'] - df_enhanced['t2_rating_std']\n",
    "\n",
    "# 5. Sniper advantage (teams with more snipers)\n",
    "t1_sniper_cols = [col for col in df_enhanced.columns if 't1_player' in col and 'is_sniper' in col]\n",
    "t2_sniper_cols = [col for col in df_enhanced.columns if 't2_player' in col and 'is_sniper' in col]\n",
    "\n",
    "if len(t1_sniper_cols) > 0 and len(t2_sniper_cols) > 0:\n",
    "    df_enhanced['t1_sniper_count'] = df_enhanced[t1_sniper_cols].sum(axis=1)\n",
    "    df_enhanced['t2_sniper_count'] = df_enhanced[t2_sniper_cols].sum(axis=1)\n",
    "    df_enhanced['sniper_diff'] = df_enhanced['t1_sniper_count'] - df_enhanced['t2_sniper_count']\n",
    "\n",
    "print(\"Additional features created:\")\n",
    "new_enhanced_features = [col for col in df_enhanced.columns if col not in df_features.columns]\n",
    "print(new_enhanced_features)\n",
    "print(f\"\\nTotal new features: {len(new_enhanced_features)}\")\n",
    "\n",
    "# Note: To use these features, you would need to:\n",
    "# 1. Re-prepare X with these new features\n",
    "# 2. Re-scale\n",
    "# 3. Re-train models\n",
    "# This is left as an exercise for further experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Improvements - Attempting Higher Accuracy\n",
    "\n",
    "We'll now try more advanced techniques to push accuracy beyond 57.77%:\n",
    "1. **XGBoost** - State-of-the-art gradient boosting\n",
    "2. **Gradient Boosting** - Alternative boosting method\n",
    "3. **Voting Ensemble** - Combine multiple models\n",
    "4. **Extended Hyperparameter Tuning** - More thorough search\n",
    "5. **Polynomial Features** - Capture feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model 4: XGBoost (if available)\n",
    "# XGBoost is often the best-performing model for structured data\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"âœ“ XGBoost is available\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"âš  XGBoost not available. Install with: !pip install xgboost\")\n",
    "    print(\"   Continuing with other models...\")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Training XGBoost Classifier...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # XGBoost with default parameters\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    \n",
    "    if len(np.unique(y_test)) > 2:\n",
    "        xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "        xgb_precision = precision_score(y_test, y_pred_xgb, average='macro', zero_division=0)\n",
    "        xgb_recall = recall_score(y_test, y_pred_xgb, average='macro', zero_division=0)\n",
    "        xgb_f1 = f1_score(y_test, y_pred_xgb, average='macro', zero_division=0)\n",
    "    else:\n",
    "        xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "        xgb_precision = precision_score(y_test, y_pred_xgb, average='binary', zero_division=0)\n",
    "        xgb_recall = recall_score(y_test, y_pred_xgb, average='binary', zero_division=0)\n",
    "        xgb_f1 = f1_score(y_test, y_pred_xgb, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nXGBoost Results:\")\n",
    "    print(f\"Accuracy: {xgb_accuracy:.4f} ({xgb_accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {xgb_precision:.4f}\")\n",
    "    print(f\"Recall: {xgb_recall:.4f}\")\n",
    "    print(f\"F1-Score: {xgb_f1:.4f}\")\n",
    "    print(f\"\\nImprovement over baseline LR: {((xgb_accuracy - lr_accuracy) * 100):.2f}%\")\n",
    "    if 'rf_tuned_accuracy' in globals():\n",
    "        print(f\"Improvement over Tuned RF: {((xgb_accuracy - rf_tuned_accuracy) * 100):.2f}%\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance_xgb = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (XGBoost):\")\n",
    "    print(feature_importance_xgb.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nSkipping XGBoost (not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model 5: Gradient Boosting (scikit-learn)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Gradient Boosting Classifier...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "    gb_precision = precision_score(y_test, y_pred_gb, average='macro', zero_division=0)\n",
    "    gb_recall = recall_score(y_test, y_pred_gb, average='macro', zero_division=0)\n",
    "    gb_f1 = f1_score(y_test, y_pred_gb, average='macro', zero_division=0)\n",
    "else:\n",
    "    gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "    gb_precision = precision_score(y_test, y_pred_gb, average='binary', zero_division=0)\n",
    "    gb_recall = recall_score(y_test, y_pred_gb, average='binary', zero_division=0)\n",
    "    gb_f1 = f1_score(y_test, y_pred_gb, average='binary', zero_division=0)\n",
    "\n",
    "print(f\"\\nGradient Boosting Results:\")\n",
    "print(f\"Accuracy: {gb_accuracy:.4f} ({gb_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {gb_precision:.4f}\")\n",
    "print(f\"Recall: {gb_recall:.4f}\")\n",
    "print(f\"F1-Score: {gb_f1:.4f}\")\n",
    "print(f\"\\nImprovement over baseline LR: {((gb_accuracy - lr_accuracy) * 100):.2f}%\")\n",
    "if 'rf_tuned_accuracy' in globals():\n",
    "    print(f\"Improvement over Tuned RF: {((gb_accuracy - rf_tuned_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model 6: Voting Ensemble (combine multiple models)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Voting Ensemble Classifier...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a voting classifier with our best models\n",
    "estimators = [\n",
    "    ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, min_samples_split=20, random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42))\n",
    "]\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    estimators.append(('xgb', xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1, eval_metric='logloss')))\n",
    "\n",
    "voting_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "    voting_precision = precision_score(y_test, y_pred_voting, average='macro', zero_division=0)\n",
    "    voting_recall = recall_score(y_test, y_pred_voting, average='macro', zero_division=0)\n",
    "    voting_f1 = f1_score(y_test, y_pred_voting, average='macro', zero_division=0)\n",
    "else:\n",
    "    voting_accuracy = accuracy_score(y_test, y_pred_voting)\n",
    "    voting_precision = precision_score(y_test, y_pred_voting, average='binary', zero_division=0)\n",
    "    voting_recall = recall_score(y_test, y_pred_voting, average='binary', zero_division=0)\n",
    "    voting_f1 = f1_score(y_test, y_pred_voting, average='binary', zero_division=0)\n",
    "\n",
    "print(f\"\\nVoting Ensemble Results:\")\n",
    "print(f\"Accuracy: {voting_accuracy:.4f} ({voting_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {voting_precision:.4f}\")\n",
    "print(f\"Recall: {voting_recall:.4f}\")\n",
    "print(f\"F1-Score: {voting_f1:.4f}\")\n",
    "print(f\"\\nImprovement over baseline LR: {((voting_accuracy - lr_accuracy) * 100):.2f}%\")\n",
    "if 'rf_tuned_accuracy' in globals():\n",
    "    print(f\"Improvement over Tuned RF: {((voting_accuracy - rf_tuned_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model 7: Extended Hyperparameter Tuning for XGBoost (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Extended Hyperparameter Tuning for XGBoost...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # More extensive parameter grid for XGBoost\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    print(\"Performing Grid Search for XGBoost (this may take several minutes)...\")\n",
    "    xgb_grid = GridSearchCV(\n",
    "        xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "        xgb_param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest Parameters: {xgb_grid.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score: {xgb_grid.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_xgb_tuned = xgb_grid.predict(X_test)\n",
    "    \n",
    "    if len(np.unique(y_test)) > 2:\n",
    "        xgb_tuned_accuracy = accuracy_score(y_test, y_pred_xgb_tuned)\n",
    "        xgb_tuned_precision = precision_score(y_test, y_pred_xgb_tuned, average='macro', zero_division=0)\n",
    "        xgb_tuned_recall = recall_score(y_test, y_pred_xgb_tuned, average='macro', zero_division=0)\n",
    "        xgb_tuned_f1 = f1_score(y_test, y_pred_xgb_tuned, average='macro', zero_division=0)\n",
    "    else:\n",
    "        xgb_tuned_accuracy = accuracy_score(y_test, y_pred_xgb_tuned)\n",
    "        xgb_tuned_precision = precision_score(y_test, y_pred_xgb_tuned, average='binary', zero_division=0)\n",
    "        xgb_tuned_recall = recall_score(y_test, y_pred_xgb_tuned, average='binary', zero_division=0)\n",
    "        xgb_tuned_f1 = f1_score(y_test, y_pred_xgb_tuned, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nTuned XGBoost Results:\")\n",
    "    print(f\"Test Accuracy: {xgb_tuned_accuracy:.4f} ({xgb_tuned_accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {xgb_tuned_precision:.4f}\")\n",
    "    print(f\"Recall: {xgb_tuned_recall:.4f}\")\n",
    "    print(f\"F1-Score: {xgb_tuned_f1:.4f}\")\n",
    "    print(f\"\\nImprovement over baseline LR: {((xgb_tuned_accuracy - lr_accuracy) * 100):.2f}%\")\n",
    "    if 'rf_tuned_accuracy' in globals():\n",
    "        print(f\"Improvement over Tuned RF: {((xgb_tuned_accuracy - rf_tuned_accuracy) * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"\\nSkipping XGBoost tuning (XGBoost not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL COMPREHENSIVE MODEL COMPARISON\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lr_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Collect all model results\u001b[39;00m\n\u001b[0;32m      7\u001b[0m all_model_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      9\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTuned Random Forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[43mlr_accuracy\u001b[49m, dt_accuracy, rf_accuracy, \n\u001b[0;32m     11\u001b[0m                  rf_tuned_accuracy \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_tuned_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     12\u001b[0m                  gb_accuracy \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan],\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m: [lr_precision, dt_precision, rf_precision,\n\u001b[0;32m     14\u001b[0m                   rf_tuned_precision \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_tuned_precision\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     15\u001b[0m                   gb_precision \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb_precision\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan],\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m: [lr_recall, dt_recall, rf_recall,\n\u001b[0;32m     17\u001b[0m                rf_tuned_recall \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_tuned_recall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     18\u001b[0m                gb_recall \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb_recall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan],\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-Score\u001b[39m\u001b[38;5;124m'\u001b[39m: [lr_f1, dt_f1, rf_f1,\n\u001b[0;32m     20\u001b[0m                  rf_tuned_f1 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_tuned_f1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m     21\u001b[0m                  gb_f1 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb_f1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan]\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Add XGBoost if available\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m XGBOOST_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Final Comparison of ALL Models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all model results\n",
    "all_model_results = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', \n",
    "              'Tuned Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy, rf_accuracy, \n",
    "                 rf_tuned_accuracy if 'rf_tuned_accuracy' in globals() else np.nan,\n",
    "                 gb_accuracy if 'gb_accuracy' in globals() else np.nan],\n",
    "    'Precision': [lr_precision, dt_precision, rf_precision,\n",
    "                  rf_tuned_precision if 'rf_tuned_precision' in globals() else np.nan,\n",
    "                  gb_precision if 'gb_precision' in globals() else np.nan],\n",
    "    'Recall': [lr_recall, dt_recall, rf_recall,\n",
    "               rf_tuned_recall if 'rf_tuned_recall' in globals() else np.nan,\n",
    "               gb_recall if 'gb_recall' in globals() else np.nan],\n",
    "    'F1-Score': [lr_f1, dt_f1, rf_f1,\n",
    "                 rf_tuned_f1 if 'rf_tuned_f1' in globals() else np.nan,\n",
    "                 gb_f1 if 'gb_f1' in globals() else np.nan]\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE and 'xgb_accuracy' in globals():\n",
    "    all_model_results['Model'].append('XGBoost')\n",
    "    all_model_results['Accuracy'].append(xgb_accuracy)\n",
    "    all_model_results['Precision'].append(xgb_precision)\n",
    "    all_model_results['Recall'].append(xgb_recall)\n",
    "    all_model_results['F1-Score'].append(xgb_f1)\n",
    "    \n",
    "    if 'xgb_tuned_accuracy' in globals():\n",
    "        all_model_results['Model'].append('Tuned XGBoost')\n",
    "        all_model_results['Accuracy'].append(xgb_tuned_accuracy)\n",
    "        all_model_results['Precision'].append(xgb_tuned_precision)\n",
    "        all_model_results['Recall'].append(xgb_tuned_recall)\n",
    "        all_model_results['F1-Score'].append(xgb_tuned_f1)\n",
    "\n",
    "# Add Voting Ensemble\n",
    "if 'voting_accuracy' in globals():\n",
    "    all_model_results['Model'].append('Voting Ensemble')\n",
    "    all_model_results['Accuracy'].append(voting_accuracy)\n",
    "    all_model_results['Precision'].append(voting_precision)\n",
    "    all_model_results['Recall'].append(voting_recall)\n",
    "    all_model_results['F1-Score'].append(voting_f1)\n",
    "\n",
    "final_comparison = pd.DataFrame(all_model_results)\n",
    "final_comparison = final_comparison.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Accuracy comparison\n",
    "models_list = final_comparison['Model'].tolist()\n",
    "accuracies = final_comparison['Accuracy'].tolist()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models_list)))\n",
    "\n",
    "axes[0].barh(models_list, accuracies, color=colors)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('All Models - Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim([0.45, max([a for a in accuracies if not np.isnan(a)]) * 1.1])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, acc in enumerate(accuracies):\n",
    "    if not np.isnan(acc):\n",
    "        axes[0].text(acc + 0.005, i, f'{acc:.4f} ({acc*100:.2f}%)', va='center', fontsize=10)\n",
    "\n",
    "# Metrics comparison for top 5 models\n",
    "top_models = final_comparison.nlargest(5, 'Accuracy')\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.15\n",
    "\n",
    "for i, (idx, row) in enumerate(top_models.iterrows()):\n",
    "    model_metrics = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]\n",
    "    axes[1].bar(x + i*width, model_metrics, width, label=row['Model'][:25], alpha=0.8)\n",
    "\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('Top 5 Models - Detailed Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x + width * 2)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = final_comparison['Accuracy'].idxmax()\n",
    "best_model_name = final_comparison.loc[best_model_idx, 'Model']\n",
    "best_model_acc = final_comparison.loc[best_model_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"   Accuracy: {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   Precision: {final_comparison.loc[best_model_idx, 'Precision']:.4f}\")\n",
    "print(f\"   Recall: {final_comparison.loc[best_model_idx, 'Recall']:.4f}\")\n",
    "print(f\"   F1-Score: {final_comparison.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
    "print(f\"\\n   Improvement over baseline LR: {((best_model_acc - lr_accuracy) * 100):.2f}%\")\n",
    "if 'rf_tuned_accuracy' in globals():\n",
    "    print(f\"   Improvement over previous best (Tuned RF): {((best_model_acc - rf_tuned_accuracy) * 100):.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing Beyond 60% - Advanced Techniques\n",
    "\n",
    "We'll now try even more sophisticated techniques to push accuracy above 60%:\n",
    "1. **Stacking Ensemble** - Use model predictions as features\n",
    "2. **Polynomial Features** - Capture feature interactions\n",
    "3. **LightGBM** - Alternative gradient boosting (often faster and sometimes better)\n",
    "4. **Extended Feature Engineering** - More sophisticated feature creation\n",
    "5. **Advanced Hyperparameter Tuning** - Larger search spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Technique 1: Stacking Ensemble\n",
    "# Use predictions from base models as features for a meta-model\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Stacking Ensemble...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create base models\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=15, min_samples_split=20, random_state=42, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42))\n",
    "]\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE and 'xgb_model' in globals():\n",
    "    base_models.append(('xgb', xgb_model))\n",
    "\n",
    "# Generate meta-features using cross-validation (to avoid overfitting)\n",
    "print(\"Generating meta-features using cross-validation...\")\n",
    "meta_features_train = np.zeros((X_train.shape[0], len(base_models)))\n",
    "meta_features_test = np.zeros((X_test.shape[0], len(base_models)))\n",
    "\n",
    "for i, (name, model) in enumerate(base_models):\n",
    "    print(f\"  Training {name} for meta-features...\")\n",
    "    # Use cross-validation to get out-of-fold predictions\n",
    "    meta_features_train[:, i] = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba')[:, 1]\n",
    "    # Train on full training set and predict on test\n",
    "    model.fit(X_train, y_train)\n",
    "    meta_features_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Train meta-model on meta-features\n",
    "from sklearn.linear_model import LogisticRegression as MetaLR\n",
    "meta_model = MetaLR(random_state=42, max_iter=1000)\n",
    "meta_model.fit(meta_features_train, y_train)\n",
    "y_pred_stacking = meta_model.predict(meta_features_test)\n",
    "\n",
    "if len(np.unique(y_test)) > 2:\n",
    "    stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "    stacking_precision = precision_score(y_test, y_pred_stacking, average='macro', zero_division=0)\n",
    "    stacking_recall = recall_score(y_test, y_pred_stacking, average='macro', zero_division=0)\n",
    "    stacking_f1 = f1_score(y_test, y_pred_stacking, average='macro', zero_division=0)\n",
    "else:\n",
    "    stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "    stacking_precision = precision_score(y_test, y_pred_stacking, average='binary', zero_division=0)\n",
    "    stacking_recall = recall_score(y_test, y_pred_stacking, average='binary', zero_division=0)\n",
    "    stacking_f1 = f1_score(y_test, y_pred_stacking, average='binary', zero_division=0)\n",
    "\n",
    "print(f\"\\nStacking Ensemble Results:\")\n",
    "print(f\"Accuracy: {stacking_accuracy:.4f} ({stacking_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {stacking_precision:.4f}\")\n",
    "print(f\"Recall: {stacking_recall:.4f}\")\n",
    "print(f\"F1-Score: {stacking_f1:.4f}\")\n",
    "if 'xgb_tuned_accuracy' in globals():\n",
    "    print(f\"\\nImprovement over Tuned XGBoost: {((stacking_accuracy - xgb_tuned_accuracy) * 100):.2f}%\")\n",
    "elif 'rf_tuned_accuracy' in globals():\n",
    "    print(f\"\\nImprovement over Tuned RF: {((stacking_accuracy - rf_tuned_accuracy) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Technique 2: LightGBM (if available)\n",
    "# LightGBM is often faster and sometimes more accurate than XGBoost\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "    print(\"âœ“ LightGBM is available\")\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"âš  LightGBM not available. Install with: !pip install lightgbm\")\n",
    "    print(\"   Continuing with other models...\")\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Training LightGBM Classifier...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # LightGBM with optimized parameters\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_pred_lgb = lgb_model.predict(X_test)\n",
    "    \n",
    "    if len(np.unique(y_test)) > 2:\n",
    "        lgb_accuracy = accuracy_score(y_test, y_pred_lgb)\n",
    "        lgb_precision = precision_score(y_test, y_pred_lgb, average='macro', zero_division=0)\n",
    "        lgb_recall = recall_score(y_test, y_pred_lgb, average='macro', zero_division=0)\n",
    "        lgb_f1 = f1_score(y_test, y_pred_lgb, average='macro', zero_division=0)\n",
    "    else:\n",
    "        lgb_accuracy = accuracy_score(y_test, y_pred_lgb)\n",
    "        lgb_precision = precision_score(y_test, y_pred_lgb, average='binary', zero_division=0)\n",
    "        lgb_recall = recall_score(y_test, y_pred_lgb, average='binary', zero_division=0)\n",
    "        lgb_f1 = f1_score(y_test, y_pred_lgb, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nLightGBM Results:\")\n",
    "    print(f\"Accuracy: {lgb_accuracy:.4f} ({lgb_accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {lgb_precision:.4f}\")\n",
    "    print(f\"Recall: {lgb_recall:.4f}\")\n",
    "    print(f\"F1-Score: {lgb_f1:.4f}\")\n",
    "    if 'xgb_tuned_accuracy' in globals():\n",
    "        print(f\"\\nImprovement over Tuned XGBoost: {((lgb_accuracy - xgb_tuned_accuracy) * 100):.2f}%\")\n",
    "    elif 'rf_tuned_accuracy' in globals():\n",
    "        print(f\"\\nImprovement over Tuned RF: {((lgb_accuracy - rf_tuned_accuracy) * 100):.2f}%\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance_lgb = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (LightGBM):\")\n",
    "    print(feature_importance_lgb.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nSkipping LightGBM (not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Technique 3: Polynomial Features (for key interactions)\n",
    "# Create polynomial features for the most important features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Polynomial Features for Top Features...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get top features from XGBoost if available, otherwise from Random Forest\n",
    "if XGBOOST_AVAILABLE and 'xgb_model' in globals():\n",
    "    top_features_idx = np.argsort(xgb_model.feature_importances_)[-20:]  # Top 20 features\n",
    "    top_feature_names = X.columns[top_features_idx].tolist()\n",
    "elif 'rf_model' in globals():\n",
    "    top_features_idx = np.argsort(rf_model.feature_importances_)[-20:]\n",
    "    top_feature_names = X.columns[top_features_idx].tolist()\n",
    "else:\n",
    "    # Use correlation-based selection\n",
    "    correlations = X_train.corrwith(pd.Series(y_train, index=X_train.index)).abs()\n",
    "    top_feature_names = correlations.nlargest(20).index.tolist()\n",
    "\n",
    "print(f\"Selected top {len(top_feature_names)} features for polynomial expansion\")\n",
    "\n",
    "# Create polynomial features (degree 2) for top features only\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_top = X_train[top_feature_names]\n",
    "X_test_top = X_test[top_feature_names]\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train_top)\n",
    "X_test_poly = poly.transform(X_test_top)\n",
    "\n",
    "print(f\"Original features: {X_train_top.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Combine original features with polynomial features\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "\n",
    "# Convert to sparse if needed for memory efficiency\n",
    "X_train_combined = np.hstack([X_train.values, X_train_poly])\n",
    "X_test_combined = np.hstack([X_test.values, X_test_poly])\n",
    "\n",
    "print(f\"Combined features: {X_train_combined.shape[1]}\")\n",
    "\n",
    "# Train XGBoost on combined features (if available)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\nTraining XGBoost on combined features...\")\n",
    "    xgb_poly = xgb.XGBClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    xgb_poly.fit(X_train_combined, y_train)\n",
    "    y_pred_xgb_poly = xgb_poly.predict(X_test_combined)\n",
    "    \n",
    "    if len(np.unique(y_test)) > 2:\n",
    "        xgb_poly_accuracy = accuracy_score(y_test, y_pred_xgb_poly)\n",
    "        xgb_poly_precision = precision_score(y_test, y_pred_xgb_poly, average='macro', zero_division=0)\n",
    "        xgb_poly_recall = recall_score(y_test, y_pred_xgb_poly, average='macro', zero_division=0)\n",
    "        xgb_poly_f1 = f1_score(y_test, y_pred_xgb_poly, average='macro', zero_division=0)\n",
    "    else:\n",
    "        xgb_poly_accuracy = accuracy_score(y_test, y_pred_xgb_poly)\n",
    "        xgb_poly_precision = precision_score(y_test, y_pred_xgb_poly, average='binary', zero_division=0)\n",
    "        xgb_poly_recall = recall_score(y_test, y_pred_xgb_poly, average='binary', zero_division=0)\n",
    "        xgb_poly_f1 = f1_score(y_test, y_pred_xgb_poly, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nXGBoost with Polynomial Features Results:\")\n",
    "    print(f\"Accuracy: {xgb_poly_accuracy:.4f} ({xgb_poly_accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {xgb_poly_precision:.4f}\")\n",
    "    print(f\"Recall: {xgb_poly_recall:.4f}\")\n",
    "    print(f\"F1-Score: {xgb_poly_f1:.4f}\")\n",
    "    if 'xgb_tuned_accuracy' in globals():\n",
    "        print(f\"\\nImprovement over Tuned XGBoost: {((xgb_poly_accuracy - xgb_tuned_accuracy) * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"\\nSkipping polynomial XGBoost (XGBoost not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Technique 4: Extended Hyperparameter Tuning for LightGBM (if available)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Extended Hyperparameter Tuning for LightGBM...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # More extensive parameter grid for LightGBM\n",
    "    lgb_param_grid = {\n",
    "        'n_estimators': [200, 300],\n",
    "        'max_depth': [5, 7, 9],\n",
    "        'learning_rate': [0.03, 0.05, 0.1],\n",
    "        'subsample': [0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    print(\"Performing Grid Search for LightGBM (this may take several minutes)...\")\n",
    "    lgb_grid = GridSearchCV(\n",
    "        lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        lgb_param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lgb_grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest Parameters: {lgb_grid.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Score: {lgb_grid.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred_lgb_tuned = lgb_grid.predict(X_test)\n",
    "    \n",
    "    if len(np.unique(y_test)) > 2:\n",
    "        lgb_tuned_accuracy = accuracy_score(y_test, y_pred_lgb_tuned)\n",
    "        lgb_tuned_precision = precision_score(y_test, y_pred_lgb_tuned, average='macro', zero_division=0)\n",
    "        lgb_tuned_recall = recall_score(y_test, y_pred_lgb_tuned, average='macro', zero_division=0)\n",
    "        lgb_tuned_f1 = f1_score(y_test, y_pred_lgb_tuned, average='macro', zero_division=0)\n",
    "    else:\n",
    "        lgb_tuned_accuracy = accuracy_score(y_test, y_pred_lgb_tuned)\n",
    "        lgb_tuned_precision = precision_score(y_test, y_pred_lgb_tuned, average='binary', zero_division=0)\n",
    "        lgb_tuned_recall = recall_score(y_test, y_pred_lgb_tuned, average='binary', zero_division=0)\n",
    "        lgb_tuned_f1 = f1_score(y_test, y_pred_lgb_tuned, average='binary', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nTuned LightGBM Results:\")\n",
    "    print(f\"Test Accuracy: {lgb_tuned_accuracy:.4f} ({lgb_tuned_accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {lgb_tuned_precision:.4f}\")\n",
    "    print(f\"Recall: {lgb_tuned_recall:.4f}\")\n",
    "    print(f\"F1-Score: {lgb_tuned_f1:.4f}\")\n",
    "    if 'xgb_tuned_accuracy' in globals():\n",
    "        print(f\"\\nImprovement over Tuned XGBoost: {((lgb_tuned_accuracy - xgb_tuned_accuracy) * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"\\nSkipping LightGBM tuning (LightGBM not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Comparison - ALL Advanced Models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPREHENSIVE MODEL COMPARISON (ALL TECHNIQUES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all model results including new advanced models\n",
    "all_advanced_results = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', \n",
    "              'Tuned Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy, rf_accuracy, \n",
    "                 rf_tuned_accuracy if 'rf_tuned_accuracy' in globals() else np.nan,\n",
    "                 gb_accuracy if 'gb_accuracy' in globals() else np.nan],\n",
    "    'Precision': [lr_precision, dt_precision, rf_precision,\n",
    "                  rf_tuned_precision if 'rf_tuned_precision' in globals() else np.nan,\n",
    "                  gb_precision if 'gb_precision' in globals() else np.nan],\n",
    "    'Recall': [lr_recall, dt_recall, rf_recall,\n",
    "               rf_tuned_recall if 'rf_tuned_recall' in globals() else np.nan,\n",
    "               gb_recall if 'gb_recall' in globals() else np.nan],\n",
    "    'F1-Score': [lr_f1, dt_f1, rf_f1,\n",
    "                 rf_tuned_f1 if 'rf_tuned_f1' in globals() else np.nan,\n",
    "                 gb_f1 if 'gb_f1' in globals() else np.nan]\n",
    "}\n",
    "\n",
    "# Add XGBoost models\n",
    "if XGBOOST_AVAILABLE and 'xgb_accuracy' in globals():\n",
    "    all_advanced_results['Model'].append('XGBoost')\n",
    "    all_advanced_results['Accuracy'].append(xgb_accuracy)\n",
    "    all_advanced_results['Precision'].append(xgb_precision)\n",
    "    all_advanced_results['Recall'].append(xgb_recall)\n",
    "    all_advanced_results['F1-Score'].append(xgb_f1)\n",
    "    \n",
    "    if 'xgb_tuned_accuracy' in globals():\n",
    "        all_advanced_results['Model'].append('Tuned XGBoost')\n",
    "        all_advanced_results['Accuracy'].append(xgb_tuned_accuracy)\n",
    "        all_advanced_results['Precision'].append(xgb_tuned_precision)\n",
    "        all_advanced_results['Recall'].append(xgb_tuned_recall)\n",
    "        all_advanced_results['F1-Score'].append(xgb_tuned_f1)\n",
    "    \n",
    "    if 'xgb_poly_accuracy' in globals():\n",
    "        all_advanced_results['Model'].append('XGBoost + Polynomial')\n",
    "        all_advanced_results['Accuracy'].append(xgb_poly_accuracy)\n",
    "        all_advanced_results['Precision'].append(xgb_poly_precision)\n",
    "        all_advanced_results['Recall'].append(xgb_poly_recall)\n",
    "        all_advanced_results['F1-Score'].append(xgb_poly_f1)\n",
    "\n",
    "# Add LightGBM models\n",
    "if LIGHTGBM_AVAILABLE and 'lgb_accuracy' in globals():\n",
    "    all_advanced_results['Model'].append('LightGBM')\n",
    "    all_advanced_results['Accuracy'].append(lgb_accuracy)\n",
    "    all_advanced_results['Precision'].append(lgb_precision)\n",
    "    all_advanced_results['Recall'].append(lgb_recall)\n",
    "    all_advanced_results['F1-Score'].append(lgb_f1)\n",
    "    \n",
    "    if 'lgb_tuned_accuracy' in globals():\n",
    "        all_advanced_results['Model'].append('Tuned LightGBM')\n",
    "        all_advanced_results['Accuracy'].append(lgb_tuned_accuracy)\n",
    "        all_advanced_results['Precision'].append(lgb_tuned_precision)\n",
    "        all_advanced_results['Recall'].append(lgb_tuned_recall)\n",
    "        all_advanced_results['F1-Score'].append(lgb_tuned_f1)\n",
    "\n",
    "# Add ensemble models\n",
    "if 'voting_accuracy' in globals():\n",
    "    all_advanced_results['Model'].append('Voting Ensemble')\n",
    "    all_advanced_results['Accuracy'].append(voting_accuracy)\n",
    "    all_advanced_results['Precision'].append(voting_precision)\n",
    "    all_advanced_results['Recall'].append(voting_recall)\n",
    "    all_advanced_results['F1-Score'].append(voting_f1)\n",
    "\n",
    "if 'stacking_accuracy' in globals():\n",
    "    all_advanced_results['Model'].append('Stacking Ensemble')\n",
    "    all_advanced_results['Accuracy'].append(stacking_accuracy)\n",
    "    all_advanced_results['Precision'].append(stacking_precision)\n",
    "    all_advanced_results['Recall'].append(stacking_recall)\n",
    "    all_advanced_results['F1-Score'].append(stacking_f1)\n",
    "\n",
    "final_advanced_comparison = pd.DataFrame(all_advanced_results)\n",
    "final_advanced_comparison = final_advanced_comparison.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(final_advanced_comparison.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Accuracy comparison\n",
    "models_list = final_advanced_comparison['Model'].tolist()\n",
    "accuracies = final_advanced_comparison['Accuracy'].tolist()\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(models_list)))\n",
    "\n",
    "axes[0].barh(models_list, accuracies, color=colors)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('All Models - Accuracy Comparison (Including Advanced Techniques)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim([0.45, max([a for a in accuracies if not np.isnan(a)]) * 1.15])\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "axes[0].axvline(x=0.60, color='red', linestyle='--', linewidth=2, label='60% Target')\n",
    "axes[0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, acc in enumerate(accuracies):\n",
    "    if not np.isnan(acc):\n",
    "        axes[0].text(acc + 0.005, i, f'{acc:.4f} ({acc*100:.2f}%)', va='center', fontsize=9)\n",
    "\n",
    "# Metrics comparison for top 5 models\n",
    "top_models = final_advanced_comparison.nlargest(5, 'Accuracy')\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.15\n",
    "\n",
    "for i, (idx, row) in enumerate(top_models.iterrows()):\n",
    "    model_metrics = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]\n",
    "    axes[1].bar(x + i*width, model_metrics, width, label=row['Model'][:25], alpha=0.8)\n",
    "\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('Top 5 Models - Detailed Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x + width * 2)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = final_advanced_comparison['Accuracy'].idxmax()\n",
    "best_model_name = final_advanced_comparison.loc[best_model_idx, 'Model']\n",
    "best_model_acc = final_advanced_comparison.loc[best_model_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"   Accuracy: {best_model_acc:.4f} ({best_model_acc*100:.2f}%)\")\n",
    "print(f\"   Precision: {final_advanced_comparison.loc[best_model_idx, 'Precision']:.4f}\")\n",
    "print(f\"   Recall: {final_advanced_comparison.loc[best_model_idx, 'Recall']:.4f}\")\n",
    "print(f\"   F1-Score: {final_advanced_comparison.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
    "print(f\"\\n   Improvement over baseline LR: {((best_model_acc - lr_accuracy) * 100):.2f}%\")\n",
    "if 'xgb_tuned_accuracy' in globals():\n",
    "    print(f\"   Improvement over Tuned XGBoost: {((best_model_acc - xgb_tuned_accuracy) * 100):.2f}%\")\n",
    "\n",
    "# Check if we reached 60%\n",
    "if best_model_acc >= 0.60:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! Achieved {best_model_acc*100:.2f}% accuracy - Above 60% target!\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Current best: {best_model_acc*100:.2f}% - {((0.60 - best_model_acc) * 100):.2f}% away from 60% target\")\n",
    "    print(f\"   This is still excellent performance for CSGO match prediction!\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Improvements\n",
    "\n",
    "**Techniques Applied:**\n",
    "1. âœ… Random Forest - Ensemble method combining multiple trees\n",
    "2. âœ… Feature Selection - Using only the most important features\n",
    "3. âœ… Hyperparameter Tuning - Grid Search for optimal parameters\n",
    "4. âœ… Additional Feature Engineering - Creating difference and interaction features\n",
    "\n",
    "**Expected Improvements:**\n",
    "- Random Forest typically improves over single Decision Tree by 2-5%\n",
    "- Feature selection can improve performance by reducing noise\n",
    "- Hyperparameter tuning can add 1-3% improvement\n",
    "- Better feature engineering can add 2-5% improvement\n",
    "\n",
    "**Total Potential Improvement:** 5-15% over baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'K-Means (ARI)'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy, ari_score],\n",
    "    'Precision': [lr_precision, dt_precision, np.nan],\n",
    "    'Recall': [lr_recall, dt_recall, np.nan],\n",
    "    'F1-Score': [lr_f1, dt_f1, np.nan],\n",
    "    'Silhouette': [np.nan, np.nan, silhouette]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(comparison_df['Model'][:2], comparison_df['Accuracy'][:2], \n",
    "            color=['skyblue', 'lightgreen'])\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Metrics comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "lr_metrics = [lr_accuracy, lr_precision, lr_recall, lr_f1]\n",
    "dt_metrics = [dt_accuracy, dt_precision, dt_recall, dt_f1]\n",
    "\n",
    "axes[1].bar(x - width/2, lr_metrics, width, label='Logistic Regression', alpha=0.8)\n",
    "axes[1].bar(x + width/2, dt_metrics, width, label='Decision Tree', alpha=0.8)\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Detailed Metrics Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results Summary\n",
    "\n",
    "This section provides a comprehensive summary of all model results and key findings from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CSGO MATCH PREDICTION - COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset Information\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total number of matches: {len(df_features)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Target variable: winner (Team 1 or Team 2)\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Data Preprocessing Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Missing values handled: {df_processed.isnull().sum().sum()} missing values after imputation\")\n",
    "print(f\"âœ“ Features scaled: {X_scaled.shape[1]} features normalized using StandardScaler\")\n",
    "print(f\"âœ“ Feature engineering: Created {len(new_features)} new aggregated features\")\n",
    "print(f\"  - Team average ratings, impact, KDR\")\n",
    "print(f\"  - Rank difference, points difference, H2H advantage\")\n",
    "\n",
    "# Model Performance Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "results_summary = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'K-Means Clustering'],\n",
    "    'Type': ['Supervised (Classification)', 'Supervised (Classification)', 'Unsupervised (Clustering)'],\n",
    "    'Accuracy/ARI': [f\"{lr_accuracy:.4f}\", f\"{dt_accuracy:.4f}\", f\"{ari_score:.4f}\"],\n",
    "    'Precision': [f\"{lr_precision:.4f}\" if 'lr_precision' in globals() else \"N/A\", \n",
    "                  f\"{dt_precision:.4f}\" if 'dt_precision' in globals() else \"N/A\", \n",
    "                  \"N/A\"],\n",
    "    'Recall': [f\"{lr_recall:.4f}\" if 'lr_recall' in globals() else \"N/A\", \n",
    "               f\"{dt_recall:.4f}\" if 'dt_recall' in globals() else \"N/A\", \n",
    "               \"N/A\"],\n",
    "    'F1-Score': [f\"{lr_f1:.4f}\" if 'lr_f1' in globals() else \"N/A\", \n",
    "                 f\"{dt_f1:.4f}\" if 'dt_f1' in globals() else \"N/A\", \n",
    "                 \"N/A\"],\n",
    "    'Silhouette Score': [\"N/A\", \"N/A\", f\"{silhouette:.4f}\" if 'silhouette' in globals() else \"N/A\"]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_summary.to_string(index=False))\n",
    "\n",
    "# Best Model Identification\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BEST MODEL IDENTIFICATION\")\n",
    "print(\"-\"*80)\n",
    "# Find best model from all available models\n",
    "all_models = {}\n",
    "if 'lr_accuracy' in globals():\n",
    "    all_models['Logistic Regression'] = lr_accuracy\n",
    "if 'dt_accuracy' in globals():\n",
    "    all_models['Decision Tree'] = dt_accuracy\n",
    "if 'rf_accuracy' in globals():\n",
    "    all_models['Random Forest'] = rf_accuracy\n",
    "if 'lr_selected_accuracy' in globals():\n",
    "    all_models['LR with Feature Selection'] = lr_selected_accuracy\n",
    "if 'rf_tuned_accuracy' in globals():\n",
    "    all_models['Tuned Random Forest'] = rf_tuned_accuracy\n",
    "if 'gb_accuracy' in globals():\n",
    "    all_models['Gradient Boosting'] = gb_accuracy\n",
    "if 'xgb_accuracy' in globals():\n",
    "    all_models['XGBoost'] = xgb_accuracy\n",
    "if 'xgb_tuned_accuracy' in globals():\n",
    "    all_models['Tuned XGBoost'] = xgb_tuned_accuracy\n",
    "if 'voting_accuracy' in globals():\n",
    "    all_models['Voting Ensemble'] = voting_accuracy\n",
    "\n",
    "if all_models:\n",
    "    best_model_name = max(all_models, key=all_models.get)\n",
    "    best_accuracy = all_models[best_model_name]\n",
    "    print(f\"ðŸ† Best Classification Model: {best_model_name}\")\n",
    "    print(f\"   Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Print metrics for best model\n",
    "    if best_model_name == \"Logistic Regression\":\n",
    "        print(f\"   Precision: {lr_precision:.4f}\")\n",
    "        print(f\"   Recall: {lr_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {lr_f1:.4f}\")\n",
    "    elif best_model_name == \"Decision Tree\":\n",
    "        print(f\"   Precision: {dt_precision:.4f}\")\n",
    "        print(f\"   Recall: {dt_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {dt_f1:.4f}\")\n",
    "    elif best_model_name == \"Random Forest\":\n",
    "        print(f\"   Precision: {rf_precision:.4f}\")\n",
    "        print(f\"   Recall: {rf_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {rf_f1:.4f}\")\n",
    "    elif best_model_name == \"LR with Feature Selection\":\n",
    "        print(f\"   Precision: {lr_selected_precision:.4f}\")\n",
    "        print(f\"   Recall: {lr_selected_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {lr_selected_f1:.4f}\")\n",
    "    elif best_model_name == \"Tuned Random Forest\":\n",
    "        print(f\"   Precision: {rf_tuned_precision:.4f}\")\n",
    "        print(f\"   Recall: {rf_tuned_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {rf_tuned_f1:.4f}\")\n",
    "    elif best_model_name == \"Gradient Boosting\":\n",
    "        print(f\"   Precision: {gb_precision:.4f}\")\n",
    "        print(f\"   Recall: {gb_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {gb_f1:.4f}\")\n",
    "    elif best_model_name == \"XGBoost\":\n",
    "        print(f\"   Precision: {xgb_precision:.4f}\")\n",
    "        print(f\"   Recall: {xgb_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {xgb_f1:.4f}\")\n",
    "    elif best_model_name == \"Tuned XGBoost\":\n",
    "        print(f\"   Precision: {xgb_tuned_precision:.4f}\")\n",
    "        print(f\"   Recall: {xgb_tuned_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {xgb_tuned_f1:.4f}\")\n",
    "    elif best_model_name == \"Voting Ensemble\":\n",
    "        print(f\"   Precision: {voting_precision:.4f}\")\n",
    "        print(f\"   Recall: {voting_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {voting_f1:.4f}\")\n",
    "    \n",
    "    # Show improvement\n",
    "    if 'lr_accuracy' in globals():\n",
    "        improvement = ((best_accuracy - lr_accuracy) * 100)\n",
    "        print(f\"\\n   Improvement over baseline LR: {improvement:+.2f}%\")\n",
    "\n",
    "# Key Insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. KEY INSIGHTS AND FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'feature_importance' in globals():\n",
    "    print(\"\\nðŸ“Š Top 5 Most Important Features (from Decision Tree):\")\n",
    "    top_5_features = feature_importance.head(5)\n",
    "    for idx, row in top_5_features.iterrows():\n",
    "        print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Model Comparison Insights:\")\n",
    "if 'lr_accuracy' in globals() and 'dt_accuracy' in globals():\n",
    "    print(f\"   â€¢ Logistic Regression provides a linear baseline with {lr_accuracy*100:.2f}% accuracy\")\n",
    "    print(f\"   â€¢ Decision Tree captures non-linear patterns with {dt_accuracy*100:.2f}% accuracy\")\n",
    "    if dt_accuracy > lr_accuracy:\n",
    "        print(f\"   â€¢ Decision Tree performs better, suggesting non-linear relationships in the data\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Logistic Regression performs better, suggesting linear relationships dominate\")\n",
    "\n",
    "if 'ari_score' in globals():\n",
    "    print(f\"   â€¢ K-Means clustering shows {ari_score:.4f} Adjusted Rand Index with actual labels\")\n",
    "    if ari_score > 0.5:\n",
    "        print(f\"   â€¢ Good cluster separation - clusters align well with match outcomes\")\n",
    "    elif ari_score > 0.3:\n",
    "        print(f\"   â€¢ Moderate cluster separation - some alignment with match outcomes\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Weak cluster separation - clusters don't strongly align with outcomes\")\n",
    "\n",
    "# Data Quality Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Original dataset shape: {df.shape}\")\n",
    "print(f\"âœ“ Processed dataset shape: {df_features.shape}\")\n",
    "print(f\"âœ“ Training samples: {X_train.shape[0]}\")\n",
    "print(f\"âœ“ Test samples: {X_test.shape[0]}\")\n",
    "print(f\"âœ“ Feature count: {X_train.shape[1]}\")\n",
    "\n",
    "# Ethical Considerations Reminder\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. ETHICAL CONSIDERATIONS REMINDER\")\n",
    "print(\"=\"*80)\n",
    "print(\"âš ï¸  Key Biases Identified:\")\n",
    "print(\"   1. Temporal Bias - Meta-game changes over time\")\n",
    "print(\"   2. Team Representation Bias - Uneven team distribution\")\n",
    "print(\"   3. Ranking System Bias - Rankings may not reflect current strength\")\n",
    "print(\"   4. Data Collection Bias - Limited tournament/region coverage\")\n",
    "print(\"   5. Player Performance Bias - Individual stats vs team dynamics\")\n",
    "print(\"\\nðŸ’¡ Mitigation strategies documented in Section 1.1.5\")\n",
    "\n",
    "# Final Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ“ Use the best performing model ({}) for match predictions\".format(best_model if 'best_model' in locals() else \"Decision Tree\"))\n",
    "print(\"âœ“ Regularly retrain models with recent data to account for meta-game changes\")\n",
    "print(\"âœ“ Monitor model performance across different teams and time periods\")\n",
    "print(\"âœ“ Consider ensemble methods combining multiple models for improved accuracy\")\n",
    "print(\"âœ“ Validate predictions with domain experts before deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF SUMMARY\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Interpretation\n",
    "\n",
    "**Logistic Regression:**\n",
    "- Provides a linear decision boundary\n",
    "- Good baseline model with interpretable coefficients\n",
    "- Performance: [Interpret results]\n",
    "\n",
    "**Decision Tree:**\n",
    "- Captures non-linear relationships\n",
    "- Feature importance reveals key predictors\n",
    "- Performance: [Interpret results]\n",
    "\n",
    "**K-Means Clustering:**\n",
    "- Unsupervised approach to identify patterns\n",
    "- Clusters may correspond to winning/losing teams\n",
    "- Performance: [Interpret results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.5 Ethical Considerations (5%)\n",
    "\n",
    "### Potential Biases and Fairness Issues\n",
    "\n",
    "1. **Temporal Bias**: \n",
    "   - **Issue**: The dataset spans multiple years (2016-2017+), and the CSGO meta-game evolves over time. Models trained on older data may not generalize to current gameplay.\n",
    "   - **Impact**: Predictions may favor strategies or team compositions that were effective in the past but are no longer relevant.\n",
    "\n",
    "2. **Team Representation Bias**:\n",
    "   - **Issue**: Some teams may appear more frequently in the dataset than others, leading to overfitting to specific team characteristics.\n",
    "   - **Impact**: The model may perform better on frequently-seen teams and poorly on underrepresented teams.\n",
    "\n",
    "3. **Ranking System Bias**:\n",
    "   - **Issue**: World rankings may not accurately reflect current team strength, especially for teams that have recently changed rosters or strategies.\n",
    "   - **Impact**: Models relying heavily on rankings may make incorrect predictions for teams in transition.\n",
    "\n",
    "4. **Data Collection Bias**:\n",
    "   - **Issue**: The dataset may only include matches from certain tournaments or regions, missing data from other competitive scenes.\n",
    "   - **Impact**: The model may not generalize well to matches from underrepresented regions or tournament types.\n",
    "\n",
    "5. **Player Performance Bias**:\n",
    "   - **Issue**: Individual player statistics may not account for team chemistry, role changes, or temporary performance fluctuations.\n",
    "   - **Impact**: Models may overvalue individual performance metrics while undervaluing team dynamics.\n",
    "\n",
    "### Mitigation Strategies\n",
    "\n",
    "1. **Temporal Validation**:\n",
    "   - Implement time-based cross-validation to ensure the model performs well across different time periods.\n",
    "   - Regularly retrain the model with recent data to adapt to meta-game changes.\n",
    "   - Consider adding temporal features (e.g., days since last match) to account for recency effects.\n",
    "\n",
    "2. **Balanced Sampling**:\n",
    "   - Use stratified sampling to ensure balanced representation of teams in training and test sets.\n",
    "   - Apply class weights during training to handle imbalanced team representation.\n",
    "   - Consider oversampling underrepresented teams or undersampling overrepresented ones.\n",
    "\n",
    "3. **Feature Engineering for Robustness**:\n",
    "   - Create relative features (e.g., rank difference, rating difference) rather than absolute values to reduce bias.\n",
    "   - Use rolling averages or recent performance metrics instead of historical averages.\n",
    "   - Include confidence intervals or uncertainty estimates in predictions.\n",
    "\n",
    "4. **Diverse Data Collection**:\n",
    "   - Ensure the dataset includes matches from various tournaments, regions, and competitive levels.\n",
    "   - Document data collection methodology and any potential gaps in coverage.\n",
    "   - Consider collecting additional metadata (tournament type, map played, etc.) to improve model fairness.\n",
    "\n",
    "5. **Model Interpretability**:\n",
    "   - Use interpretable models (like decision trees) or provide feature importance analysis.\n",
    "   - Conduct fairness audits to check if the model performs equally well across different team types.\n",
    "   - Implement model monitoring to detect performance degradation or bias introduction over time.\n",
    "\n",
    "6. **Transparency and Documentation**:\n",
    "   - Clearly document model limitations and assumptions.\n",
    "   - Provide confidence scores with predictions to indicate uncertainty.\n",
    "   - Regularly review and update the model based on new data and feedback.\n",
    "\n",
    "7. **Ethical Use Guidelines**:\n",
    "   - Ensure predictions are used responsibly and not to manipulate betting markets.\n",
    "   - Consider the impact on players and teams if predictions are made public.\n",
    "   - Respect privacy of player performance data and comply with data protection regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has implemented a complete machine learning workflow for CSGO match prediction:\n",
    "\n",
    "1. **Dataset Selection**: CSGO match dataset with comprehensive team and player statistics\n",
    "2. **Data Preprocessing**: Handled missing values, feature engineering, scaling, and encoding\n",
    "3. **Exploratory Data Analysis**: Visualizations and pattern identification\n",
    "4. **Model Development**: Trained Logistic Regression, Decision Tree, and K-Means models\n",
    "5. **Evaluation**: Comprehensive metrics and performance comparison\n",
    "6. **Ethical Considerations**: Identified biases and proposed mitigation strategies\n",
    "\n",
    "The models demonstrate [summarize performance] and can be used for match outcome prediction with appropriate considerations for the identified biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}